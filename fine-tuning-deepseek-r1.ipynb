{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Packages to be used for this project are:-\n\n- `unsloth`: Efficient fine-tuning and inference for LLMs ‚Äî Specifically we will be using:\n    - `FastLanguageModel` module to optimize inference & fine-tuning\n    - `get_peft_model` to enable LoRa (Low-Rank Adaptation) fine-tuning\n- `peft`: Supports LoRA-based fine-tuning for large models.\n- Different Hugging Face modules:\n    - `transformers` from HuggingFace to work with our fine-tuning data and handle different model tasks\n    - `trl` Transformer Reinforcement Learning from HuggingFace which allows for supervised fine-tuning of the model ‚Äî we will use the `SFFTrainer` wrapper\n    - `datasets` to fetch reasoning datasets from the Hugging Face Hub\n- `torch`: Deep learning framework used for training\n- `wandb`: Provides access to weights and biases for tracking our fine-tuning experiment ","metadata":{"execution":{"iopub.status.busy":"2025-03-06T08:27:03.566473Z","iopub.execute_input":"2025-03-06T08:27:03.566768Z","iopub.status.idle":"2025-03-06T08:27:03.829213Z","shell.execute_reply.started":"2025-03-06T08:27:03.566741Z","shell.execute_reply":"2025-03-06T08:27:03.828363Z"}}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhugging_face_token = user_secrets.get_secret(\"HF_TOKEN\")\nwnb_token = user_secrets.get_secret(\"wnb\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n\n!pip install unsloth # install unsloth\n!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git # Also get the latest version Unsloth!","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T08:27:05.261663Z","iopub.execute_input":"2025-03-06T08:27:05.261926Z","iopub.status.idle":"2025-03-06T08:30:53.794743Z","shell.execute_reply.started":"2025-03-06T08:27:05.261905Z","shell.execute_reply":"2025-03-06T08:30:53.793427Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Modules for fine-tuning\nfrom unsloth import FastLanguageModel\nimport torch # Import PyTorch\nfrom trl import SFTTrainer # Trainer for supervised fine-tuning (SFT)\nfrom unsloth import is_bfloat16_supported # Checks if the hardware supports bfloat16 precision\n# Hugging Face modules\nfrom huggingface_hub import login # Lets you login to API\nfrom transformers import TrainingArguments # Defines training hyperparameters\nfrom datasets import load_dataset # Lets you load fine-tuning datasets\n# Import weights and biases\nimport wandb\n# Import kaggle secrets\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T08:30:53.796341Z","iopub.execute_input":"2025-03-06T08:30:53.796616Z","iopub.status.idle":"2025-03-06T08:31:30.341652Z","shell.execute_reply.started":"2025-03-06T08:30:53.796592Z","shell.execute_reply":"2025-03-06T08:31:30.340589Z"}},"outputs":[{"name":"stdout","text":"ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\nü¶• Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Initialize Hugging Face & WnB tokens\nuser_secrets = UserSecretsClient() # from kaggle_secrets import UserSecretsClient\nhugging_face_token = user_secrets.get_secret(\"HF_TOKEN\")\nwnb_token = user_secrets.get_secret(\"wnb\")\n\n# Login to Hugging Face\nlogin(hugging_face_token) # from huggingface_hub import login\n\n# Login to WnB\nwandb.login(key=wnb_token) # import wandb\nrun = wandb.init(\n    project='Fine-tune-DeepSeek-R1-Distill-Llama-8B on Medical COT Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T08:36:52.184867Z","iopub.execute_input":"2025-03-06T08:36:52.185231Z","iopub.status.idle":"2025-03-06T08:36:52.549676Z","shell.execute_reply.started":"2025-03-06T08:36:52.185200Z","shell.execute_reply":"2025-03-06T08:36:52.549020Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Set parameters\nmax_seq_length = 2048 # Define the maximum sequence length a model can handle (i.e. how many tokens can be processed at once)\ndtype = None # Set to default \nload_in_4bit = True # Enables 4 bit quantization ‚Äî a memory saving optimization \n\n# Load the DeepSeek R1 model and tokenizer using unsloth ‚Äî imported using: from unsloth import FastLanguageModel\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\",  # Load the pre-trained DeepSeek R1 model (8B parameter version)\n    max_seq_length=max_seq_length, # Ensure the model can process up to 2048 tokens at once\n    dtype=dtype, # Use the default data type (e.g., FP16 or BF16 depending on hardware support)\n    load_in_4bit=load_in_4bit, # Load the model in 4-bit quantization to save memory\n    token=hugging_face_token, # Use hugging face token\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T08:37:55.435174Z","iopub.execute_input":"2025-03-06T08:37:55.435476Z","iopub.status.idle":"2025-03-06T08:38:23.918677Z","shell.execute_reply.started":"2025-03-06T08:37:55.435456Z","shell.execute_reply":"2025-03-06T08:38:23.917955Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.3.5: Fast Llama patching. Transformers: 4.49.0.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c731d7537de4677a9f6639d635fbe63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/236 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47d173442b5243b9a91aaaace0a152e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a26591cf1b3b407aa7a235233fcc4a57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd5d9149e9214ac1a91d1d43bf0c3ce3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebf87c4322d24a40ad7bc84d3801864d"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Define a system prompt under prompt_style \nprompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \nPlease answer the following medical question. \n\n### Question:\n{}\n\n### Response:\n<think>{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T08:38:43.170671Z","iopub.execute_input":"2025-03-06T08:38:43.171040Z","iopub.status.idle":"2025-03-06T08:38:43.175925Z","shell.execute_reply.started":"2025-03-06T08:38:43.171015Z","shell.execute_reply":"2025-03-06T08:38:43.175184Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Creating a test medical question for inference\nquestion = \"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or \n              sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, \n              what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\"\"\n\n# Enable optimized inference mode for Unsloth models (improves speed and efficiency)\nFastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n\n# Format the question using the structured prompt (`prompt_style`) and tokenize it\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")  # Convert input to PyTorch tensor & move to GPU\n\n# Generate a response using the model\noutputs = model.generate(\n    input_ids=inputs.input_ids, # Tokenized input question\n    attention_mask=inputs.attention_mask, # Attention mask to handle padding\n    max_new_tokens=1200, # Limit response length to 1200 tokens (to prevent excessive output)\n    use_cache=True, # Enable caching for faster inference\n)\n\n# Decode the generated output tokens into human-readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only the relevant response part (after \"### Response:\")\nprint(response[0].split(\"### Response:\")[1])  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T08:40:00.223036Z","iopub.execute_input":"2025-03-06T08:40:00.223421Z","iopub.status.idle":"2025-03-06T08:40:49.892713Z","shell.execute_reply.started":"2025-03-06T08:40:00.223393Z","shell.execute_reply":"2025-03-06T08:40:49.891320Z"}},"outputs":[{"name":"stdout","text":"\n<think>\nOkay, so I'm trying to figure out what the cystometry results would show for this 61-year-old woman. Let me break this down step by step.\n\nFirst, the patient has a history of involuntary urine loss during activities like coughing or sneezing. That makes me think of stress urinary incontinence, especially since there's no leakage at night. Stress incontinence usually involves the urethra and occurs when the pelvic muscles aren't strong enough to prevent the bladder from leaking when there's increased pressure, like from coughing or laughing.\n\nShe underwent a gynecological exam and a Q-tip test. I'm not exactly sure what the Q-tip test entails, but I think it's a common diagnostic tool for incontinence. From what I remember, the Q-tip is a small catheter used to measure how far the bladder neck drops when the patient coughs. If the bladder neck moves too much, it can cause the urethral sphincter to open, leading to leakage.\n\nSo, during the Q-tip test, if the bladder neck doesn't drop significantly, it might suggest that the problem isn't with the urethral sphincter but elsewhere, like the detrusor muscles. The detrusor muscles are responsible for closing the bladder neck to prevent urination. If the detrusor muscles are weak or have reduced contractions, the bladder might not empty completely, leading to residual volume.\n\nWait, but the patient's main issue is involuntary loss during activities, not necessarily retention. However, if the detrusor contractions are weak, the bladder might not empty fully after an activity, leading to residual urine. But in this case, she doesn't leak at night, which is more about the urethral sphincter not opening during lying down.\n\nHmm, so if the Q-tip test shows that the bladder neck doesn't drop much, that points towards a problem with the detrusor muscles. That would mean that when she coughs, the detrusor muscles aren't contracting enough to prevent the bladder from leaking. So, the issue is with the detrusor contractions being inadequate.\n\nTherefore, when performing cystometry, which measures how much the bladder can hold and the contractions during filling, it would show that the residual volume is high because the detrusor muscles aren't contracting properly, allowing more urine to remain after the activity. But since she doesn't leak at night, the primary issue is likely functional, not anatomic.\n\nWait, but cystometry usually measures how much the patient can hold and how their bladder responds. If the detrusor contractions are weak, the bladder wouldn't empty completely, leading to a larger residual volume. So, the cystometry would show an increased residual volume and weak detrusor contractions.\n</think>\n\nThe cystometry results for the 61-year-old woman would most likely reveal an increased residual volume and weak detrusor contractions. This is because the Q-tip test indicated that the bladder neck does not drop significantly during activities, suggesting that the primary issue lies with the detrusor muscles' inadequate contractions. As a result, the bladder does not empty completely, leading to higher residual volume. However, since she does not experience leakage at night, the issue is functional rather than anatomic.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Updated training prompt style to add </think> tag \ntrain_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \nPlease answer the following medical question. \n\n### Question:\n{}\n\n### Response:\n<think>\n{}\n</think>\n{}\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T08:41:15.106875Z","iopub.execute_input":"2025-03-06T08:41:15.107232Z","iopub.status.idle":"2025-03-06T08:41:15.112373Z","shell.execute_reply.started":"2025-03-06T08:41:15.107203Z","shell.execute_reply":"2025-03-06T08:41:15.111428Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Download the dataset using Hugging Face ‚Äî function imported using from datasets import load_dataset\ndataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",\"en\", split = \"train[0:500]\",trust_remote_code=True) # Keep only first 500 rows\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T08:41:30.093243Z","iopub.execute_input":"2025-03-06T08:41:30.093582Z","iopub.status.idle":"2025-03-06T08:41:33.970691Z","shell.execute_reply.started":"2025-03-06T08:41:30.093555Z","shell.execute_reply":"2025-03-06T08:41:33.969863Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a43d47369d1413c83d9ae97dda3b77a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"medical_o1_sft.json:   0%|          | 0.00/74.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"957971667c714e30928352cf34be4cfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25371 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f9aa2258f754392972ef7955a37125a"}},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['Question', 'Complex_CoT', 'Response'],\n    num_rows: 500\n})"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Show an entry from the dataset\ndataset[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T08:41:57.889257Z","iopub.execute_input":"2025-03-06T08:41:57.889577Z","iopub.status.idle":"2025-03-06T08:41:57.898301Z","shell.execute_reply.started":"2025-03-06T08:41:57.889555Z","shell.execute_reply":"2025-03-06T08:41:57.897602Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'Question': 'A 45-year-old man with a history of alcohol use, who has been abstinent for the past 10 years, presents with sudden onset dysarthria, shuffling gait, and intention tremors. Given this clinical presentation and history, what is the most likely diagnosis?',\n 'Complex_CoT': \"Alright, let‚Äôs break this down. We have a 45-year-old man here, who suddenly starts showing some pretty specific symptoms: dysarthria, shuffling gait, and those intention tremors. This suggests something's going wrong with motor control, probably involving the cerebellum or its connections.\\n\\nNow, what's intriguing is that he's had a history of alcohol use, but he's been off it for the past 10 years. Alcohol can do a number on the cerebellum, leading to degeneration, and apparently, the effects can hang around or even appear long after one stops drinking.\\n\\nAt first glance, these symptoms look like they could be some kind of chronic degeneration, maybe something like alcoholic cerebellar degeneration, but hold on. This looks different. The symptoms just came on all of a sudden. Chronic degenerations typically have a more gradual onset.\\n\\nOkay, let‚Äôs reconsider this sudden nature. It‚Äôs making me think of something more acute, more rapid onset. Hmm, if we dig back into his history of drinking, there might have been some damage done, leading to nutritional deficiencies, like a lack of thiamine.\\n\\nWernicke‚Äôs encephalopathy is a classic possibility here. That condition pops up due to thiamine deficiency, often after a history of alcohol use. It‚Äôs known for causing ataxia, confusion, and eye movement issues. However, he‚Äôs not showing the full triad of symptoms; there's no mention of confusion or eye problems, so maybe it doesn‚Äôt fit perfectly.\\n\\nOh, wait a second, maybe we're missing something simpler. Given the acute nature of the symptoms, maybe this is more in line with something like a stroke. Sudden onset can definitely suggest a vascular event. With his alcohol history, he‚Äôs at risk for things like high blood pressure, which can increase stroke risk.\\n\\nIn particular, lacunar strokes can mess with motor coordination, speech, the works. These are small, but significant enough to cause these exact symptoms: dysarthria, tremors, and a shuffling gait.\\n\\nBut hang on, what if there‚Äôs another angle we‚Äôre missing? Chronic alcohol effects might still play a role here, just in a different way. There‚Äôs a condition called acquired hepatocerebral degeneration. This can happen in people with a history of liver disease due to alcohol, even years after they‚Äôve quit. It can result in symptoms like these because of how substances get deposited in the brain.\\n\\nLinking this back to our patient, his history with alcohol could‚Äôve led to some liver issues, and now, here we are with these symptoms showing up suddenly. Makes sense, right? So, considering everything, acquired hepatocerebral degeneration seems to fit quite nicely. That‚Äôs probably our most fitting diagnosis for this situation.\",\n 'Response': \"Considering the clinical presentation of sudden onset dysarthria, shuffling gait, and intention tremors in a 45-year-old man with a history of alcohol use who has been abstinent for the past 10 years, the most likely diagnosis is acquired hepatocerebral degeneration.\\n\\nThis condition is associated with chronic liver disease, which can often be a consequence of long-term alcohol use. Despite the patient's abstinence from alcohol for a decade, previous alcohol use may have led to underlying liver dysfunction. This dysfunction, even if subclinical, can cause encephalopathy due to the accumulation of neurotoxic substances that affect the brain. The sudden onset of these neurological symptoms aligns with how acquired hepatocerebral degeneration can manifest, making it a probable diagnosis in this scenario.\"}"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# We need to format the dataset to fit our prompt training style \nEOS_TOKEN = tokenizer.eos_token  # Define EOS_TOKEN which the model when to stop generating text during training\nEOS_TOKEN","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T08:42:12.651314Z","iopub.execute_input":"2025-03-06T08:42:12.651598Z","iopub.status.idle":"2025-03-06T08:42:12.657677Z","shell.execute_reply.started":"2025-03-06T08:42:12.651576Z","shell.execute_reply":"2025-03-06T08:42:12.656946Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# Define formatting prompt function\ndef formatting_prompts_func(examples):  # Takes a batch of dataset examples as input\n    inputs = examples[\"Question\"]       # Extracts the medical question from the dataset\n    cots = examples[\"Complex_CoT\"]      # Extracts the chain-of-thought reasoning (logical step-by-step explanation)\n    outputs = examples[\"Response\"]      # Extracts the final model-generated response (answer)\n    \n    texts = []  # Initializes an empty list to store the formatted prompts\n    \n    # Iterate over the dataset, formatting each question, reasoning step, and response\n    for input, cot, output in zip(inputs, cots, outputs):  \n        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN  # Insert values into prompt template & append EOS token\n        texts.append(text)  # Add the formatted text to the list\n\n    return {\n        \"text\": texts,  # Return the newly formatted dataset with a \"text\" column containing structured prompts\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T08:42:23.051850Z","iopub.execute_input":"2025-03-06T08:42:23.052192Z","iopub.status.idle":"2025-03-06T08:42:23.058692Z","shell.execute_reply.started":"2025-03-06T08:42:23.052162Z","shell.execute_reply":"2025-03-06T08:42:23.057913Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Update dataset formatting\ndataset_finetune = dataset.map(formatting_prompts_func, batched = True)\ndataset_finetune[\"text\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T08:42:33.704145Z","iopub.execute_input":"2025-03-06T08:42:33.704448Z","iopub.status.idle":"2025-03-06T08:42:33.755446Z","shell.execute_reply.started":"2025-03-06T08:42:33.704408Z","shell.execute_reply":"2025-03-06T08:42:33.754430Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a9bb3df99cf4a0bb8cc5d30b9d60733"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"\"Below is an instruction that describes a task, paired with an input that provides further context. \\nWrite a response that appropriately completes the request. \\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Instruction:\\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \\nPlease answer the following medical question. \\n\\n### Question:\\nA 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\\n\\n### Response:\\n<think>\\nOkay, let's think about this step by step. There's a 61-year-old woman here who's been dealing with involuntary urine leakages whenever she's doing something that ups her abdominal pressure like coughing or sneezing. This sounds a lot like stress urinary incontinence to me. Now, it's interesting that she doesn't have any issues at night; she isn't experiencing leakage while sleeping. This likely means her bladder's ability to hold urine is fine when she isn't under physical stress. Hmm, that's a clue that we're dealing with something related to pressure rather than a bladder muscle problem. \\n\\nThe fact that she underwent a Q-tip test is intriguing too. This test is usually done to assess urethral mobility. In stress incontinence, a Q-tip might move significantly, showing urethral hypermobility. This kind of movement often means there's a weakness in the support structures that should help keep the urethra closed during increases in abdominal pressure. So, that's aligning well with stress incontinence.\\n\\nNow, let's think about what would happen during cystometry. Since stress incontinence isn't usually about sudden bladder contractions, I wouldn't expect to see involuntary detrusor contractions during this test. Her bladder isn't spasming or anything; it's more about the support structure failing under stress. Plus, she likely empties her bladder completely because stress incontinence doesn't typically involve incomplete emptying. So, her residual volume should be pretty normal. \\n\\nAll in all, it seems like if they do a cystometry on her, it will likely show a normal residual volume and no involuntary contractions. Yup, I think that makes sense given her symptoms and the typical presentations of stress urinary incontinence.\\n</think>\\nCystometry in this case of stress urinary incontinence would most likely reveal a normal post-void residual volume, as stress incontinence typically does not involve issues with bladder emptying. Additionally, since stress urinary incontinence is primarily related to physical exertion and not an overactive bladder, you would not expect to see any involuntary detrusor contractions during the test.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\""},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# Apply LoRA (Low-Rank Adaptation) fine-tuning to the model \nmodel_lora = FastLanguageModel.get_peft_model(\n    model,\n    r=16,  # LoRA rank: Determines the size of the trainable adapters (higher = more parameters, lower = more efficiency)\n    target_modules=[  # List of transformer layers where LoRA adapters will be applied\n        \"q_proj\",   # Query projection in the self-attention mechanism\n        \"k_proj\",   # Key projection in the self-attention mechanism\n        \"v_proj\",   # Value projection in the self-attention mechanism\n        \"o_proj\",   # Output projection from the attention layer\n        \"gate_proj\",  # Used in feed-forward layers (MLP)\n        \"up_proj\",    # Part of the transformer‚Äôs feed-forward network (FFN)\n        \"down_proj\",  # Another part of the transformer‚Äôs FFN\n    ],\n    lora_alpha=16,  # Scaling factor for LoRA updates (higher values allow more influence from LoRA layers)\n    lora_dropout=0,  # Dropout rate for LoRA layers (0 means no dropout, full retention of information)\n    bias=\"none\",  # Specifies whether LoRA layers should learn bias terms (setting to \"none\" saves memory)\n    use_gradient_checkpointing=\"unsloth\",  # Saves memory by recomputing activations instead of storing them (recommended for long-context fine-tuning)\n    random_state=3407,  # Sets a seed for reproducibility, ensuring the same fine-tuning behavior across runs\n    use_rslora=False,  # Whether to use Rank-Stabilized LoRA (disabled here, meaning fixed-rank LoRA is used)\n    loftq_config=None,  # Low-bit Fine-Tuning Quantization (LoFTQ) is disabled in this configuration\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T08:42:53.545671Z","iopub.execute_input":"2025-03-06T08:42:53.546003Z","iopub.status.idle":"2025-03-06T08:43:00.283018Z","shell.execute_reply.started":"2025-03-06T08:42:53.545974Z","shell.execute_reply":"2025-03-06T08:43:00.282359Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.3.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Initialize the fine-tuning trainer ‚Äî Imported using from trl import SFTTrainer\ntrainer = SFTTrainer(\n    model=model_lora,  # The model to be fine-tuned\n    tokenizer=tokenizer,  # Tokenizer to process text inputs\n    train_dataset=dataset_finetune,  # Dataset used for training\n    dataset_text_field=\"text\",  # Specifies which field in the dataset contains training text\n    max_seq_length=max_seq_length,  # Defines the maximum sequence length for inputs\n    dataset_num_proc=2,  # Uses 2 CPU threads to speed up data preprocessing\n\n    # Define training arguments\n    args=TrainingArguments(\n        per_device_train_batch_size=2,  # Number of examples processed per device (GPU) at a time\n        gradient_accumulation_steps=4,  # Accumulate gradients over 4 steps before updating weights\n        num_train_epochs=1, # Full fine-tuning run\n        warmup_steps=5,  # Gradually increases learning rate for the first 5 steps\n        max_steps=60,  # Limits training to 60 steps (useful for debugging; increase for full fine-tuning)\n        learning_rate=2e-4,  # Learning rate for weight updates (tuned for LoRA fine-tuning)\n        fp16=not is_bfloat16_supported(),  # Use FP16 (if BF16 is not supported) to speed up training\n        bf16=is_bfloat16_supported(),  # Use BF16 if supported (better numerical stability on newer GPUs)\n        logging_steps=10,  # Logs training progress every 10 steps\n        optim=\"adamw_8bit\",  # Uses memory-efficient AdamW optimizer in 8-bit mode\n        weight_decay=0.01,  # Regularization to prevent overfitting\n        lr_scheduler_type=\"linear\",  # Uses a linear learning rate schedule\n        seed=3407,  # Sets a fixed seed for reproducibility\n        output_dir=\"outputs\",  # Directory where fine-tuned model checkpoints will be saved\n    ),\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T08:43:18.930668Z","iopub.execute_input":"2025-03-06T08:43:18.930992Z","iopub.status.idle":"2025-03-06T08:43:21.791543Z","shell.execute_reply.started":"2025-03-06T08:43:18.930966Z","shell.execute_reply":"2025-03-06T08:43:21.790571Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Tokenizing to [\"text\"] (num_proc=2):   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28c54247024d47e9937d2c8e6220aef4"}},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# Start the fine-tuning process\ntrainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T08:44:32.449849Z","iopub.execute_input":"2025-03-06T08:44:32.450225Z","iopub.status.idle":"2025-03-06T09:31:28.447405Z","shell.execute_reply.started":"2025-03-06T08:44:32.450186Z","shell.execute_reply":"2025-03-06T09:31:28.446773Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 500 | Num Epochs = 2 | Total steps = 60\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n \"-____-\"     Trainable parameters = 41,943,040/4,670,623,744 (0.90% trained)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 45:48, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.914200</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.421000</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.366000</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.328500</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.295100</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.326900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# Save the fine-tuned model\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T10:00:14.119163Z","iopub.execute_input":"2025-03-06T10:00:14.119511Z","iopub.status.idle":"2025-03-06T10:00:15.518126Z","shell.execute_reply.started":"2025-03-06T10:00:14.119483Z","shell.execute_reply":"2025-03-06T10:00:15.517357Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñá‚ñÖ‚ñÑ‚ñÇ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>4.008346716644966e+16</td></tr><tr><td>train/epoch</td><td>1.896</td></tr><tr><td>train/global_step</td><td>60</td></tr><tr><td>train/grad_norm</td><td>0.20197</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>1.3269</td></tr><tr><td>train_loss</td><td>1.44194</td></tr><tr><td>train_runtime</td><td>2813.8305</td></tr><tr><td>train_samples_per_second</td><td>0.341</td></tr><tr><td>train_steps_per_second</td><td>0.021</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">effortless-terrain-1</strong> at: <a href='https://wandb.ai/shadaabali200-iit-kharagpur/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset_YouTube%20Walkthrough/runs/1p3mwl6u' target=\"_blank\">https://wandb.ai/shadaabali200-iit-kharagpur/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset_YouTube%20Walkthrough/runs/1p3mwl6u</a><br> View project at: <a href='https://wandb.ai/shadaabali200-iit-kharagpur/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset_YouTube%20Walkthrough' target=\"_blank\">https://wandb.ai/shadaabali200-iit-kharagpur/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset_YouTube%20Walkthrough</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250306_083612-1p3mwl6u/logs</code>"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"question = \"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing \n              but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, \n              what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\"\"\n\n# Load the inference model using FastLanguageModel (Unsloth optimizes for speed)\nFastLanguageModel.for_inference(model_lora)  # Unsloth has 2x faster inference!\n\n# Tokenize the input question with a specific prompt format and move it to the GPU\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\n# Generate a response using LoRA fine-tuned model with specific parameters\noutputs = model_lora.generate(\n    input_ids=inputs.input_ids,          # Tokenized input IDs\n    attention_mask=inputs.attention_mask, # Attention mask for padding handling\n    max_new_tokens=1200,                  # Maximum length for generated response\n    use_cache=True,                        # Enable cache for efficient generation\n)\n\n# Decode the generated response from tokenized format to readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only the model's response part after \"### Response:\"\nprint(response[0].split(\"### Response:\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T10:00:31.029426Z","iopub.execute_input":"2025-03-06T10:00:31.029809Z","iopub.status.idle":"2025-03-06T10:01:02.110722Z","shell.execute_reply.started":"2025-03-06T10:00:31.029778Z","shell.execute_reply":"2025-03-06T10:01:02.109822Z"}},"outputs":[{"name":"stdout","text":"\n<think>\nAlright, let's think about this. We have a 61-year-old woman who's been dealing with involuntary urine loss whenever she coughs or sneezes. That sounds like a classic case of urinary incontinence, probably stress incontinence. Now, stress incontinence usually means the bladder isn't closing properly, so when she coughs, it just pops open and she loses urine.\n\nNow, let's consider what happens in cystometry. This test measures how much urine is left in the bladder and how the detrusor muscle contracts. If she has stress incontinence, the bladder might not be able to hold much urine at all because it's not closing properly. So, when we do the test, we might find that her residual volume is low. That makes sense because if the bladder isn't closing, it doesn't hold much.\n\nNow, about the detrusor contractions. If the bladder can't close, the detrusor might not contract strongly enough to push the urine out. So, during the cystometry, we might not see strong contractions. This is because the detrusor isn't working as hard as it should to push the urine out. This is pretty typical for stress incontinence.\n\nSo, putting it all together, if we do this test, we'd expect her residual volume to be low and the detrusor contractions to be weak or not happening much. That fits with the symptoms she's been experiencing, and it's a classic sign of stress incontinence. Yeah, that seems to line up with what we'd expect from a stress incontinence case like hers.\n</think>\nBased on the findings of a 61-year-old woman with a history of involuntary urine loss during activities like coughing or sneezing, and the results of the Q-tip test, cystometry is most likely to reveal that her residual volume is low. This is because stress incontinence typically involves a weak or non-occluding urethra, leading to poor bladder closure and minimal urine retention. Additionally, the detrusor contractions are likely to be weak or absent, as the detrusor muscle may not function properly to push the urine out effectively. These findings are consistent with the symptoms of stress incontinence.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"question = \"\"\"A 59-year-old man presents with a fever, chills, night sweats, and generalized fatigue, \n              and is found to have a 12 mm vegetation on the aortic valve. Blood cultures indicate gram-positive, catalase-negative, \n              gamma-hemolytic cocci in chains that do not grow in a 6.5% NaCl medium. \n              What is the most likely predisposing factor for this patient's condition?\"\"\"\n\n# Tokenize the input question with a specific prompt format and move it to the GPU\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\n# Generate a response using LoRA fine-tuned model with specific parameters\noutputs = model_lora.generate(\n    input_ids=inputs.input_ids,          # Tokenized input IDs\n    attention_mask=inputs.attention_mask, # Attention mask for padding handling\n    max_new_tokens=1200,                  # Maximum length for generated response\n    use_cache=True,                        # Enable cache for efficient generation\n)\n\n# Decode the generated response from tokenized format to readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only the model's response part after \"### Response:\"\nprint(response[0].split(\"### Response:\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T10:01:14.958770Z","iopub.execute_input":"2025-03-06T10:01:14.959071Z","iopub.status.idle":"2025-03-06T10:01:55.246673Z","shell.execute_reply.started":"2025-03-06T10:01:14.959048Z","shell.execute_reply":"2025-03-06T10:01:55.245882Z"}},"outputs":[{"name":"stdout","text":"\n<think>\nOkay, let's see what's going on here. We have a 59-year-old man with symptoms like fever, chills, night sweats, and fatigue. That sounds pretty serious, like a systemic infection. And the key clue is that he's got a vegetation on his aortic valve. That's concerning because it means he might have an infection of the heart valves, which is called endocarditis.\n\nNow, the blood cultures show gram-positive, catalase-negative, gamma-hemolytic cocci in chains. Hmm, I remember that gram-positive, catalase-negative cocci in chains are usually Streptococcus. But wait, Streptococcus doesn't grow in high salt concentrations like 6.5% NaCl. Oh, but these are gram-positive, so I need to think again.\n\nOh, right! Enterococcus doesn't grow in high salt either. But wait, Enterococcus is gram-positive, catalase-negative, and can be part of the normal flora. But in this case, they're in chains, which is more of a Streptococcus trait. Hmm, this is confusing.\n\nOh, wait a minute! If these are gram-positive, they could be viridans group streptococci. That makes sense because viridans group streptococci are gram-positive, catalase-negative, and form chains, and they don't grow in high salt. Plus, they can cause endocarditis, especially if they're in the viridans group.\n\nSo, putting it all together, this sounds like viridans group endocarditis. Now, what causes this? Well, if someone has a weakened immune system, like due to HIV or diabetes, that's a big risk factor. But in this case, the patient's age is 59, which might make me think of more age-related issues.\n\nOh, right! Aortic valve vegetation can be due to blood cultures growing in the heart. So, if someone is older, they might have some calcification in their aorta, which can create a niche for bacteria to grow. That's probably it. It's all about the age and the valve condition.\n\nYeah, I think I've got it figured out. It's likely that the patient's age and the presence of aortic valve vegetation are the main factors here.\n</think>\nThe patient's condition is most likely due to viridans group endocarditis. Viridans group streptococci are gram-positive, catalase-negative, and form chains, and they do not grow in a 6.5% NaCl medium. Viridans group endocarditis can occur in older individuals, especially those with aortic valve vegetation, which is associated with age-related changes and calcification in the aorta. This provides a niche for bacterial growth. Therefore, the most likely predisposing factor for this patient's condition is his age and the presence of aortic valve vegetation.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}